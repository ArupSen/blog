<p>A friend of mine had a problem with his computer and he couldn't access anything on it. It reminded me that even though I used to be very meticulous about backing up myself, since  <a href="http://arupdesigns.co.uk/blog/archives/1021" target="_blank">creating my gaming computer</a> I had not really considered any sort of backup strategy and now that more than a year had passed I thought it was about time that I did something about it.</p>
<p>I went through my pile of spare hard drives and found one  that was the same size as the one that I currently have in my machine which is 500 GB. I formatted it using GParted as Ext4 and then set about finding a way to set up some sort of automated procedure. When I had a Mac I used to use a software called Carbon Copy Cloner which would create a bootable clone of my drive but that wasn't something that I really needed right now. Everything that I have under version control in a Git repository has a remote copy on either GitHub or Bitbucket. The music that I have I could find again but then why should I start again? I started to look through my files and found there is quite a lot that I would rather not lose and even if most of my files are available on remote servers it's still a pain to have to retrieve them all over again.</p>
<p>As an initial set up and copy of the drive that I wanted to back up I used rsync which got stuck at one point because I had not told it to exclude a directory called 'backup' which is where the backup drive was mounted. This would create a loop making a backup of the backup of the backup and so on. I used '--exclude' followed by the directory path to stop rsync from looking at it. Having excluded this one I noticed that there were others that could also be excluded such as the .cache directory which I really wouldn't miss.</p>
<p>That was the selective copying ticked off but what about automation? I did a bit of research and found that using cron was the way to do it. I set up a cron job with my rsync command and set it for 11pm.</p>
<p>After a few days I noticed that I would need the computer to be switched on for the backup task to run and that wasn't the case every day. I guess it would be OK to miss the odd day but something about this didn't sit right with me. I was setting up a daily backup task which wasn't going to run on a daily basis. I also noticed that certain files weren't being copied due to permission issues. I could use the root crontab but better still (after some further research) I found that there is a system cron directory for monthly, weekly, daily and hourly jobs in the /etc directory. That was where I headed next.</p>
<p>In these folders you don't have crontabs but you can just create a bash script of what you want to run and then place it in the folder. It would have root privileges of course and so I wouldn't have any permission issues.</p>
<p>I wanted my back up to run more frequently than daily just in case I didn't switch on my computer for the whole day and by default this runs at 17 minutes past the hour. I thought that a lot of resources would be used up during this task but I found that because I had only made small changes in the intervening periods the task would take only a few minutes. I ran htop to check this and even though the copying task used more CPU power than any other task it didn't seem to slow down the computer as such and within those 4 minutes, depending on how many changes I had made.</p>
<p>After a day or two of experimentation I settled on this hourly automated backup. Very much fit and forget. One slightly annoying thing is that GRUB thinks that it's a start up disk and makes my boot time slightly longer but then I'm usually not sitting and waiting for my computer to boot.</p>
